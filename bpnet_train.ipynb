{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bpnet_train.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPv4ymrEU03nBefiVre3x98",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvantiShri/colab_notebooks/blob/master/bpnet_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpciSvT0Kvb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "831e2011-b08f-4c67-a7db-80f24b2dbe7c"
      },
      "source": [
        "!mkdir Oct4 Nanog Sox2 Klf4 patchcap\n",
        "\n",
        "![[ -e Oct4/counts.neg.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Oct4/counts.neg.bw -O Oct4/counts.neg.bw\n",
        "![[ -e Oct4/counts.pos.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Oct4/counts.pos.bw -O Oct4/counts.pos.bw\n",
        "![[ -e Oct4/idr-optimal-set.summit.bed.gz ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Oct4/idr-optimal-set.summit.bed.gz\t -O Oct4/idr-optimal-set.summit.bed.gz\n",
        "\n",
        "![[ -e Sox2/counts.neg.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Sox2/counts.neg.bw -O Sox2/counts.neg.bw\n",
        "![[ -e Sox2/counts.pos.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Sox2/counts.pos.bw -O Sox2/counts.pos.bw\n",
        "![[ -e Sox2/idr-optimal-set.summit.bed.gz ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Sox2/idr-optimal-set.summit.bed.gz\t -O Sox2/idr-optimal-set.summit.bed.gz\n",
        "\n",
        "![[ -e Nanog/counts.neg.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Nanog/counts.neg.bw -O Nanog/counts.neg.bw\n",
        "![[ -e Nanog/counts.pos.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Nanog/counts.pos.bw -O Nanog/counts.pos.bw\n",
        "![[ -e Nanog/idr-optimal-set.summit.bed.gz ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Nanog/idr-optimal-set.summit.bed.gz\t -O Nanog/idr-optimal-set.summit.bed.gz\n",
        "\n",
        "![[ -e Klf4/counts.neg.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Klf4/counts.neg.bw -O Klf4/counts.neg.bw\n",
        "![[ -e Klf4/counts.pos.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Klf4/counts.pos.bw -O Klf4/counts.pos.bw\n",
        "![[ -e Klf4/idr-optimal-set.summit.bed.gz ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Klf4/idr-optimal-set.summit.bed.gz\t -O Klf4/idr-optimal-set.summit.bed.gz\n",
        "\n",
        "![[ -e patchcap/counts.neg.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/patchcap/counts.neg.bw -O patchcap/counts.neg.bw\n",
        "![[ -e patchcap/counts.pos.bw ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/patchcap/counts.pos.bw -O patchcap/counts.pos.bw\n",
        "\n",
        "![[ -e mm10_no_alt_analysis_set_ENCODE.fasta.gz ]] || wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/mm10_no_alt_analysis_set_ENCODE.fasta.gz -O mm10_no_alt_analysis_set_ENCODE.fasta.gz\n",
        "![[ -e mm10_no_alt_analysis_set_ENCODE.fasta ]] || gunzip -f mm10_no_alt_analysis_set_ENCODE.fasta"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-12 01:57:28--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Oct4/counts.neg.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 576815136 (550M)\n",
            "Saving to: ‘Oct4/counts.neg.bw’\n",
            "\n",
            "Oct4/counts.neg.bw  100%[===================>] 550.09M  4.69MB/s    in 88s     \n",
            "\n",
            "2020-06-12 01:58:56 (6.27 MB/s) - ‘Oct4/counts.neg.bw’ saved [576815136/576815136]\n",
            "\n",
            "--2020-06-12 01:58:57--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Oct4/counts.pos.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 576928607 (550M)\n",
            "Saving to: ‘Oct4/counts.pos.bw’\n",
            "\n",
            "Oct4/counts.pos.bw  100%[===================>] 550.20M  7.79MB/s    in 71s     \n",
            "\n",
            "2020-06-12 02:00:08 (7.75 MB/s) - ‘Oct4/counts.pos.bw’ saved [576928607/576928607]\n",
            "\n",
            "--2020-06-12 02:00:09--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Oct4/idr-optimal-set.summit.bed.gz\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 208760 (204K) [application/x-gzip]\n",
            "Saving to: ‘Oct4/idr-optimal-set.summit.bed.gz’\n",
            "\n",
            "Oct4/idr-optimal-se 100%[===================>] 203.87K   498KB/s    in 0.4s    \n",
            "\n",
            "2020-06-12 02:00:10 (498 KB/s) - ‘Oct4/idr-optimal-set.summit.bed.gz’ saved [208760/208760]\n",
            "\n",
            "--2020-06-12 02:00:11--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Sox2/counts.neg.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 314609681 (300M)\n",
            "Saving to: ‘Sox2/counts.neg.bw’\n",
            "\n",
            "Sox2/counts.neg.bw  100%[===================>] 300.04M  6.44MB/s    in 35s     \n",
            "\n",
            "2020-06-12 02:00:46 (8.61 MB/s) - ‘Sox2/counts.neg.bw’ saved [314609681/314609681]\n",
            "\n",
            "--2020-06-12 02:00:46--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Sox2/counts.pos.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 314510460 (300M)\n",
            "Saving to: ‘Sox2/counts.pos.bw’\n",
            "\n",
            "Sox2/counts.pos.bw  100%[===================>] 299.94M  9.44MB/s    in 31s     \n",
            "\n",
            "2020-06-12 02:01:18 (9.57 MB/s) - ‘Sox2/counts.pos.bw’ saved [314510460/314510460]\n",
            "\n",
            "--2020-06-12 02:01:19--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Sox2/idr-optimal-set.summit.bed.gz\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89290 (87K) [application/x-gzip]\n",
            "Saving to: ‘Sox2/idr-optimal-set.summit.bed.gz’\n",
            "\n",
            "Sox2/idr-optimal-se 100%[===================>]  87.20K   320KB/s    in 0.3s    \n",
            "\n",
            "2020-06-12 02:01:20 (320 KB/s) - ‘Sox2/idr-optimal-set.summit.bed.gz’ saved [89290/89290]\n",
            "\n",
            "--2020-06-12 02:01:20--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Nanog/counts.neg.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 416457789 (397M)\n",
            "Saving to: ‘Nanog/counts.neg.bw’\n",
            "\n",
            "Nanog/counts.neg.bw 100%[===================>] 397.17M  8.98MB/s    in 46s     \n",
            "\n",
            "2020-06-12 02:02:07 (8.65 MB/s) - ‘Nanog/counts.neg.bw’ saved [416457789/416457789]\n",
            "\n",
            "--2020-06-12 02:02:08--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Nanog/counts.pos.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 416417177 (397M)\n",
            "Saving to: ‘Nanog/counts.pos.bw’\n",
            "\n",
            "Nanog/counts.pos.bw 100%[===================>] 397.13M  8.01MB/s    in 43s     \n",
            "\n",
            "2020-06-12 02:02:50 (9.31 MB/s) - ‘Nanog/counts.pos.bw’ saved [416417177/416417177]\n",
            "\n",
            "--2020-06-12 02:02:51--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Nanog/idr-optimal-set.summit.bed.gz\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 455832 (445K) [application/x-gzip]\n",
            "Saving to: ‘Nanog/idr-optimal-set.summit.bed.gz’\n",
            "\n",
            "Nanog/idr-optimal-s 100%[===================>] 445.15K   654KB/s    in 0.7s    \n",
            "\n",
            "2020-06-12 02:02:52 (654 KB/s) - ‘Nanog/idr-optimal-set.summit.bed.gz’ saved [455832/455832]\n",
            "\n",
            "--2020-06-12 02:02:53--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Klf4/counts.neg.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 366053375 (349M)\n",
            "Saving to: ‘Klf4/counts.neg.bw’\n",
            "\n",
            "Klf4/counts.neg.bw  100%[===================>] 349.09M  10.4MB/s    in 36s     \n",
            "\n",
            "2020-06-12 02:03:29 (9.79 MB/s) - ‘Klf4/counts.neg.bw’ saved [366053375/366053375]\n",
            "\n",
            "--2020-06-12 02:03:31--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Klf4/counts.pos.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 366050360 (349M)\n",
            "Saving to: ‘Klf4/counts.pos.bw’\n",
            "\n",
            "Klf4/counts.pos.bw  100%[===================>] 349.09M  11.0MB/s    in 46s     \n",
            "\n",
            "2020-06-12 02:04:17 (7.62 MB/s) - ‘Klf4/counts.pos.bw’ saved [366050360/366050360]\n",
            "\n",
            "--2020-06-12 02:04:18--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/Klf4/idr-optimal-set.summit.bed.gz\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 464734 (454K) [application/x-gzip]\n",
            "Saving to: ‘Klf4/idr-optimal-set.summit.bed.gz’\n",
            "\n",
            "Klf4/idr-optimal-se 100%[===================>] 453.84K   668KB/s    in 0.7s    \n",
            "\n",
            "2020-06-12 02:04:19 (668 KB/s) - ‘Klf4/idr-optimal-set.summit.bed.gz’ saved [464734/464734]\n",
            "\n",
            "--2020-06-12 02:04:20--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/patchcap/counts.neg.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 160921957 (153M)\n",
            "Saving to: ‘patchcap/counts.neg.bw’\n",
            "\n",
            "patchcap/counts.neg 100%[===================>] 153.47M  4.47MB/s    in 19s     \n",
            "\n",
            "2020-06-12 02:04:40 (7.99 MB/s) - ‘patchcap/counts.neg.bw’ saved [160921957/160921957]\n",
            "\n",
            "--2020-06-12 02:04:41--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/patchcap/counts.pos.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 160925907 (153M)\n",
            "Saving to: ‘patchcap/counts.pos.bw’\n",
            "\n",
            "patchcap/counts.pos 100%[===================>] 153.47M  13.1MB/s    in 14s     \n",
            "\n",
            "2020-06-12 02:04:55 (11.0 MB/s) - ‘patchcap/counts.pos.bw’ saved [160925907/160925907]\n",
            "\n",
            "--2020-06-12 02:04:57--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/mm10_no_alt_analysis_set_ENCODE.fasta.gz\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870119834 (830M) [application/x-gzip]\n",
            "Saving to: ‘mm10_no_alt_analysis_set_ENCODE.fasta.gz’\n",
            "\n",
            "mm10_no_alt_analysi 100%[===================>] 829.81M  10.2MB/s    in 75s     \n",
            "\n",
            "2020-06-12 02:06:12 (11.1 MB/s) - ‘mm10_no_alt_analysis_set_ENCODE.fasta.gz’ saved [870119834/870119834]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_J5JpkTWvrF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "78d893ae-ea16-4131-b557-8b6bbaa8b665"
      },
      "source": [
        "!md5sum Oct4/* Sox2/* Nanog/* Klf4/* patchcap/*"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70e1f8307583509214cb72260aab6d5b  Oct4/counts.neg.bw\n",
            "d7b0458fd622842a96fb7b72a4a01af5  Oct4/counts.pos.bw\n",
            "83d9a95f94dcbb9f43de136497902400  Oct4/idr-optimal-set.summit.bed.gz\n",
            "e61c408a9c39efe37892d13d440bef2d  Sox2/counts.neg.bw\n",
            "d88d3af721a2fb9ed1356a68c0400313  Sox2/counts.pos.bw\n",
            "ec2e818f3fd15b427eab9ab332c5e317  Sox2/idr-optimal-set.summit.bed.gz\n",
            "3d7b6d1d2ecfd7192f9884d14897cd5b  Nanog/counts.neg.bw\n",
            "57adcc51ef400742869979bf021c6b50  Nanog/counts.pos.bw\n",
            "429ee4ff5270748d7feec683d94c5bc4  Nanog/idr-optimal-set.summit.bed.gz\n",
            "9b4259a13aed4d72529ec4d390f4387e  Klf4/counts.neg.bw\n",
            "238b2400d36ce1c5d5bba9aeb378a545  Klf4/counts.pos.bw\n",
            "c769cfd88f44ec854db717d81d0933a5  Klf4/idr-optimal-set.summit.bed.gz\n",
            "f82c66ee5e5a0471846db35191fe481a  patchcap/counts.neg.bw\n",
            "98d336de48cbdcd89a0e5a4c0dd43633  patchcap/counts.pos.bw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEttq-RsSHRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4bed1e1a-f8e8-4a4a-82a8-24f31068a232"
      },
      "source": [
        "!pip install seqdataloader"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqdataloader\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/19/a3ef914e583b9faf9cd5dc5807e9ee08fa7a2aa8130e97eb2c41e31d8b5e/seqdataloader-0.130-py3-none-any.whl\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from seqdataloader) (0.29.19)\n",
            "Collecting pybedtools>=0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/14/152220f39cda6b9b72810eeed103c6ec78422429adabe3aafc3eaf6feb40/pybedtools-0.8.1.tar.gz (12.5MB)\n",
            "\u001b[K     |████████████████████████████████| 12.5MB 246kB/s \n",
            "\u001b[?25hCollecting pyBigWig>=0.3.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/e2/cf945d541a10bb9c675f986d5bf0b0268544721054d17cc6260cfcfb3685/pyBigWig-0.3.17.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hCollecting pyfaidx\n",
            "  Downloading https://files.pythonhosted.org/packages/d9/eb/bca4c916d2cde775b5127cef22f276142b01e89fc31fecd832ed996dc97e/pyfaidx-0.5.8.tar.gz\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seqdataloader) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.6/dist-packages (from seqdataloader) (1.0.4)\n",
            "Collecting deeptools>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/72/c6b2fdf1ab026ff827e7b8dce72643046ae8744499eb209844aa2cbd3d75/deepTools-3.4.3.tar.gz (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pybedtools>=0.7->seqdataloader) (1.12.0)\n",
            "Collecting pysam\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/74/69018650a6ec9dae2eb2f710e158fa395134ce579c605f152e41890e7f3c/pysam-0.16.0.tar.gz (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.6/dist-packages (from pyfaidx->seqdataloader) (47.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->seqdataloader) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->seqdataloader) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from deeptools>=3.0.1->seqdataloader) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from deeptools>=3.0.1->seqdataloader) (3.2.1)\n",
            "Collecting numpydoc>=0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/43/2402fd1f63992a52f88e3b169d59674617013bf7f1ad0cd7d842eafd0c58/numpydoc-1.0.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting py2bit>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/53/bb/547a927bed736ead3dc909e1e552d57c9034bb9493eff80544c0cf6e4828/py2bit-0.3.0.tar.gz\n",
            "Requirement already satisfied: plotly>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from deeptools>=3.0.1->seqdataloader) (4.4.1)\n",
            "Collecting deeptoolsintervals>=0.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/1f/d10d6ad23c86c62d90d867d0506881a392ec6ef06885b858eaab868dd356/deeptoolsintervals-0.1.9.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->deeptools>=3.0.1->seqdataloader) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->deeptools>=3.0.1->seqdataloader) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->deeptools>=3.0.1->seqdataloader) (0.10.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (1.8.5)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=2.0.0->deeptools>=3.0.1->seqdataloader) (1.3.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (1.1.1)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (2.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (1.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (2.23.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (0.7.12)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (20.4)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (0.15.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (2.8.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (1.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader) (3.0.4)\n",
            "Building wheels for collected packages: pybedtools, pyBigWig, pyfaidx, deeptools, pysam, py2bit, deeptoolsintervals\n",
            "  Building wheel for pybedtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybedtools: filename=pybedtools-0.8.1-cp36-cp36m-linux_x86_64.whl size=13605440 sha256=b82fb70d7afca6ebdeee96d23179aba87a5bfc1aa0f834fc36b2288c217089f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/50/97/7d0e4f605d0d1578997f4bba3061869c2dee9f8cd29f626323\n",
            "  Building wheel for pyBigWig (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyBigWig: filename=pyBigWig-0.3.17-cp36-cp36m-linux_x86_64.whl size=178151 sha256=15a4b0e44f3ef02eddf9814e101b824d8350c673abe1ff0876bb7ddf5933782d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/27/2d/ac3e2e2d17894877fd3c4595ebd6fbd25ad805bfeab333f19b\n",
            "  Building wheel for pyfaidx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfaidx: filename=pyfaidx-0.5.8-cp36-none-any.whl size=25051 sha256=fc00d861907ff825bb2d2546af3a054e2ac53b27813e3c530dec09f58d0c3161\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/ea/ee/59d4649b0fb82a0690bdeae834bc85891b306126bcc067e29f\n",
            "  Building wheel for deeptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeptools: filename=deepTools-3.4.3-cp36-none-any.whl size=217274 sha256=0db6fab2d0bca99ea6eaab063898c00b2f6f953d31bf26ee99f94019dad2c11c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/d0/0c/0608998dc78250578f0c548e6792200fedc9b26ed49a388a2d\n",
            "  Building wheel for pysam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysam: filename=pysam-0.16.0-cp36-cp36m-linux_x86_64.whl size=9095481 sha256=f374865680245ff009eeebda1b68aeb10e287a9225aa3d2e6dc3203d2f890e89\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/8d/9a/d4f3608692800f194d9f9f266736dc70461331a4a6d3fd2eae\n",
            "  Building wheel for py2bit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py2bit: filename=py2bit-0.3.0-cp36-cp36m-linux_x86_64.whl size=43531 sha256=1fbe5a24d67364595fc07b841cf123e0f6a64824a45459f2f708c145cca62d77\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/66/b6/33fb9b65b31121127f1da60ca27948ecf8d4c59b0967298de8\n",
            "  Building wheel for deeptoolsintervals (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeptoolsintervals: filename=deeptoolsintervals-0.1.9-cp36-cp36m-linux_x86_64.whl size=108525 sha256=3119798c12358f00bde8ec0de3b6fde89bd4ea92eefb245a58d33b5702da758e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/60/60/e513c6246f67379f6e1b8d09448cdf913bac3851f96bd42e94\n",
            "Successfully built pybedtools pyBigWig pyfaidx deeptools pysam py2bit deeptoolsintervals\n",
            "Installing collected packages: pysam, pybedtools, pyBigWig, pyfaidx, numpydoc, py2bit, deeptoolsintervals, deeptools, seqdataloader\n",
            "Successfully installed deeptools-3.4.3 deeptoolsintervals-0.1.9 numpydoc-1.0.0 py2bit-0.3.0 pyBigWig-0.3.17 pybedtools-0.8.1 pyfaidx-0.5.8 pysam-0.16.0 seqdataloader-0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIKunE8MVmPc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9251afb9-fb86-407f-b22c-a430e69f2e72"
      },
      "source": [
        "!apt-get install bedtools"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  bedtools\n",
            "0 upgraded, 1 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 577 kB of archives.\n",
            "After this operation, 2,040 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 bedtools amd64 2.26.0+dfsg-5 [577 kB]\n",
            "Fetched 577 kB in 2s (262 kB/s)\n",
            "Selecting previously unselected package bedtools.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../bedtools_2.26.0+dfsg-5_amd64.deb ...\n",
            "Unpacking bedtools (2.26.0+dfsg-5) ...\n",
            "Setting up bedtools (2.26.0+dfsg-5) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFl5Q9zXVvY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9464baa3-30fd-4e49-a564-22d18afd6fcf"
      },
      "source": [
        "!wget http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.chrom.sizes -O mm10.chrom.sizes "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-12 02:24:09--  http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.chrom.sizes\n",
            "Resolving hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1405 (1.4K) [text/plain]\n",
            "Saving to: ‘mm10.chrom.sizes’\n",
            "\n",
            "mm10.chrom.sizes    100%[===================>]   1.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-12 02:24:10 (232 MB/s) - ‘mm10.chrom.sizes’ saved [1405/1405]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKUKoqTEVNei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get 1kb around summits\n",
        "![[ -f 1k_around_summits.bed.gz ]] || zcat Oct4/idr-optimal-set.summit.bed.gz Sox2/idr-optimal-set.summit.bed.gz Nanog/idr-optimal-set.summit.bed.gz Klf4/idr-optimal-set.summit.bed.gz | perl -lane 'print $F[0].\"\\t\".(($F[1]+$F[9])).\"\\t\".(($F[1]+$F[9]))' | bedtools slop -g mm10.chrom.sizes -b 500 | perl -lane 'if ($F[2]-$F[1]==1000) {print $F[0].\"\\t\".$F[1].\"\\t\".$F[2].\"\\t1\"}' | sortBed | gzip -c > 1k_around_summits.bed.gz\n",
        "#split into train, valid, test sets\n",
        "![[ -f test_1k_around_summits.bed.gz ]] || zcat 1k_around_summits.bed.gz | egrep -w 'chr1|chr8|chr21' | gzip -c > test_1k_around_summits.bed.gz\n",
        "![[ -f valid_1k_around_summits.bed.gz ]] || zcat 1k_around_summits.bed.gz | egrep -w 'chr22' | gzip -c > valid_1k_around_summits.bed.gz\n",
        "![[ -f train_1k_around_summits.bed.gz ]] || zcat 1k_around_summits.bed.gz | egrep -w -v 'chr1|chr8|chr21|chr22' | gzip -c > train_1k_around_summits.bed.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJwZXko2ONuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "90609ae9-394b-4cfc-c248-445a2bdc5bd9"
      },
      "source": [
        "from collections import OrderedDict\n",
        "from seqdataloader.batchproducers import coordbased\n",
        "from seqdataloader.batchproducers.coordbased import coordstovals\n",
        "from seqdataloader.batchproducers.coordbased import coordbatchproducers\n",
        "from seqdataloader.batchproducers.coordbased import coordbatchtransformers\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.core import CoordsToValsJoiner\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import AbstractCountAndProfileTransformer \n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import LogCountsPlusOne\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import SmoothProfiles\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import BigWigReader \n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import smooth_profiles\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import rolling_window\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import MultiTrackCountsAndProfile\n",
        "import keras\n",
        "import keras.layers as kl\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "from seqdataloader.batchproducers.coordbased.core import Coordinates, KerasBatchGenerator, apply_mask\n",
        "from keras.models import load_model\n",
        "import os\n",
        "from keras.utils import CustomObjectScope\n",
        "from seqdataloader.batchproducers.coordbased.core import Coordinates"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EhxOzwbO2SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Written by Žiga Avsec\n",
        "import pyBigWig\n",
        "#loss function\n",
        "def multinomial_nll(true_counts, logits):\n",
        "    \"\"\"Compute the multinomial negative log-likelihood\n",
        "    Args:\n",
        "      true_counts: observed count values\n",
        "      logits: predicted logit values\n",
        "    \"\"\"\n",
        "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
        "    dist = tfp.distributions.Multinomial(total_count=counts_per_example,\n",
        "                                         logits=logits)\n",
        "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
        "            tf.to_float(tf.shape(true_counts)[0]))\n",
        "\n",
        "#Written by Žiga Avsec\n",
        "class MultichannelMultinomialNLL(object):\n",
        "    def __init__(self, n):\n",
        "        self.__name__ = \"MultichannelMultinomialNLL\"\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, true_counts, logits):\n",
        "        for i in range(self.n):\n",
        "            loss = multinomial_nll(true_counts[..., i], logits[..., i])\n",
        "            if i == 0:\n",
        "                total = loss\n",
        "            else:\n",
        "                total += loss\n",
        "        return total\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"n\": self.n}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYITvafzSV8V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "6547a4a7-1502-430d-ddeb-118737bf9294"
      },
      "source": [
        "# If we want to avoid zero-padding, then the size of the output predictions\n",
        "# will depend on the size of the input sequence supplied. We define the\n",
        "# API for an AbstractProfileModel class which returns the length of the\n",
        "# output profile in addition to returning the model, given information\n",
        "# on the size of the input sequence and the model parameters.\n",
        "        \n",
        "class AbstractProfileModel(object):\n",
        "    \n",
        "    def get_output_profile_len(self):\n",
        "        raise NotImplementedError()\n",
        "  \n",
        "    def get_model(self):\n",
        "        raise NotImplementedError()\n",
        "  \n",
        "# The architecture by Žiga Avsec involves residual connections, which means\n",
        "# that the layers being added together in an elementwise fashion need\n",
        "# to have the same dimensions. To achieve this without zero-padding, we\n",
        "# have to trim away the flanks of earlier convolutional layers. That\n",
        "# is what this function is meant to do. (Note that the original BP-net\n",
        "# architecture zero-pads; this is a modification to avoid the zero\n",
        "# padding and use information on actual sequence instead)\n",
        "\n",
        "def trim_flanks_of_conv_layer(conv_layer, output_len, width_to_trim, filters):\n",
        "    layer = keras.layers.Lambda(\n",
        "        lambda x: x[:,\n",
        "          int(0.5*(width_to_trim)):-(width_to_trim-int(0.5*(width_to_trim)))],\n",
        "        output_shape=(output_len, filters))(conv_layer)\n",
        "    return layer\n",
        "        \n",
        "#This model architecture is based on BP-Net by Žiga Avsec\n",
        "# https://drive.google.com/file/d/1kg6Ic0-FvJtVUva9Mh3FPnOAHJcN6VB-/view\n",
        "#It has been modified for this specific use-case.\n",
        "\n",
        "class BPnetArch(AbstractProfileModel):   \n",
        "\n",
        "    def __init__(self, input_seq_len, c_task_weight, filters,\n",
        "                       n_dil_layers, conv1_kernel_size,\n",
        "                       dil_kernel_size,\n",
        "                       outconv_kernel_size, lr,\n",
        "                       seed=1234):\n",
        "        self.input_seq_len = input_seq_len\n",
        "        self.c_task_weight = c_task_weight\n",
        "        self.filters = filters\n",
        "        self.n_dil_layers = n_dil_layers\n",
        "        self.conv1_kernel_size = conv1_kernel_size\n",
        "        self.dil_kernel_size = dil_kernel_size\n",
        "        self.outconv_kernel_size = outconv_kernel_size\n",
        "        self.lr = lr\n",
        "        self.seed = seed\n",
        "    \n",
        "    def get_embedding_len(self):\n",
        "        embedding_len = self.input_seq_len\n",
        "        embedding_len -= (self.conv1_kernel_size-1)     \n",
        "        for i in range(1, self.n_dil_layers+1):\n",
        "            dilation_rate = (2**i)\n",
        "            embedding_len -= dilation_rate*(self.dil_kernel_size-1)\n",
        "        return embedding_len\n",
        "    \n",
        "    def get_output_profile_len(self):\n",
        "        embedding_len = self.get_embedding_len()\n",
        "        out_profile_len = embedding_len - (self.outconv_kernel_size - 1)\n",
        "        return out_profile_len\n",
        "    \n",
        "    def get_keras_model(self):\n",
        "      \n",
        "        np.random.seed(self.seed)\n",
        "        tf.set_random_seed(self.seed)\n",
        "\n",
        "        out_pred_len = self.get_output_profile_len()\n",
        "        \n",
        "        #'inp' is the one-hot encoded DNA sequence input\n",
        "        inp = kl.Input(shape=(self.input_seq_len, 4), name='sequence')\n",
        "        first_conv = kl.Conv1D(filters=self.filters,\n",
        "                               kernel_size=self.conv1_kernel_size,\n",
        "                               padding='valid',\n",
        "                               activation='relu')(inp)\n",
        "        \n",
        "        #Need to keep track of the layer size for trimming purposes when\n",
        "        # we get to the residual connections.\n",
        "        \n",
        "        curr_layer_size = self.input_seq_len - (self.conv1_kernel_size-1)\n",
        "        \n",
        "        #Define input layers for the control tracks - both counts and profile\n",
        "        #Dimension is '1' for the ChIP-seq control counts because the positive\n",
        "        # and negative strands are added together\n",
        "        \n",
        "        logcount_chipnexus = kl.Input(\n",
        "            shape=(2,), name=\"CHIPNexus.logcount\")\n",
        "        profile_chipnexus = kl.Input(\n",
        "            shape=(out_pred_len, 2), name=\"CHIPNexus.profile\")\n",
        "        \n",
        "        #Gather together all the tensors representing the model inputs\n",
        "        model_inputs = [\n",
        "            inp,\n",
        "            logcount_chipnexus,\n",
        "            profile_chipnexus\n",
        "        ]\n",
        "        \n",
        "        #Prepare the stack of dilated convolutions with residual connections\n",
        "        prev_layer = first_conv\n",
        "        for i in range(1, self.n_dil_layers + 1):\n",
        "          dilation_rate = 2**i\n",
        "          conv_output = kl.Conv1D(filters=self.filters,\n",
        "                                  kernel_size=self.dil_kernel_size,\n",
        "                                  padding='valid',\n",
        "                                  activation='relu',\n",
        "                                  dilation_rate=dilation_rate)(prev_layer)          \n",
        "          width_to_trim = dilation_rate*(self.dil_kernel_size-1)\n",
        "          curr_layer_size = (curr_layer_size - width_to_trim)\n",
        "          prev_layer = trim_flanks_of_conv_layer(\n",
        "              conv_layer=prev_layer, output_len=curr_layer_size,\n",
        "              width_to_trim=width_to_trim, filters=self.filters)\n",
        "          prev_layer = kl.merge.Add()([prev_layer, conv_output])\n",
        "\n",
        "        combined_conv = prev_layer\n",
        "\n",
        "        #gap = GlobalAveragePooling. This layer is used as input for the\n",
        "        # counts prediction tasks.\n",
        "        gap_combined_conv = kl.GlobalAvgPool1D()(combined_conv)\n",
        "        \n",
        "        lossarr = []\n",
        "        lossweightsarr = []\n",
        "        model_outputs = []\n",
        "        \n",
        "        #Define the output layers for the counts prediction tasks\n",
        "        for countouttaskname, numunits, countcontrolinp in [(\"CHIPNexus.NANOG.logcount\", 2, logcount_chipnexus),\n",
        "                                                            (\"CHIPNexus.OCT4.logcount\", 2, logcount_chipnexus),\n",
        "                                                            (\"CHIPNexus.KLF4.logcount\", 2, logcount_chipnexus),\n",
        "                                                            (\"CHIPNexus.SOX2.logcount\", 2, logcount_chipnexus)]:\n",
        "        \n",
        "            count_out = kl.Dense(units=numunits,\n",
        "                                name=countouttaskname)(\n",
        "                                  kl.concatenate([gap_combined_conv, countcontrolinp], axis=-1))\n",
        "            model_outputs.append(count_out)\n",
        "            lossarr.append('mse')\n",
        "            lossweightsarr.append(self.c_task_weight)\n",
        "\n",
        "        #Define the output layers for the profile prediction tasks\n",
        "        for profileouttaskname, numunits, profilecontrolinp in [(\"CHIPNexus.NANOG.profile\", 2, profile_chipnexus),\n",
        "                                                                (\"CHIPNexus.OCT4.profile\", 2, profile_chipnexus),\n",
        "                                                                (\"CHIPNexus.KLF4.profile\", 2, profile_chipnexus),\n",
        "                                                                (\"CHIPNexus.SOX2.profile\", 2, profile_chipnexus)]:\n",
        "                                                                \n",
        "        \n",
        "            profile_out_precontrol = kl.Conv1D(\n",
        "                                      filters=numunits,\n",
        "                                      kernel_size=self.outconv_kernel_size,\n",
        "                                      padding='valid')(combined_conv)\n",
        "            profile_out = kl.Conv1D(filters=numunits, kernel_size=1, name=profileouttaskname)(kl.concatenate([profile_out_precontrol,profilecontrolinp], axis=-1))\n",
        "\n",
        "            model_outputs.append(profile_out)\n",
        "            lossarr.append(MultichannelMultinomialNLL(numunits)) \n",
        "              #We downweight the loss by the number of channels because, if you\n",
        "              # read the code for MultichannelMultinomialNLL, you'll see that\n",
        "              # the loss for different channels is added together;\n",
        "              # but I (Avanti Shrikumar) didn't want to implicitly upweight\n",
        "              # the prediction tasks that happen\n",
        "              # to have more channels, hence this downweighting.\n",
        "            lossweightsarr.append(1.0/numunits)\n",
        "\n",
        "        #Compile the model and return it\n",
        "        model = keras.models.Model(inputs=model_inputs, outputs=model_outputs)\n",
        "        model.compile(keras.optimizers.Adam(lr=self.lr),\n",
        "                      loss=lossarr,\n",
        "                      loss_weights=lossweightsarr)\n",
        "        return model\n",
        "\n",
        "seq_len = 1346\n",
        "\n",
        "modelwrapper = BPnetArch(\n",
        "    input_seq_len=seq_len, c_task_weight=500,\n",
        "    filters=64, n_dil_layers=6,\n",
        "    conv1_kernel_size=21,\n",
        "    dil_kernel_size=3,\n",
        "    outconv_kernel_size=75,\n",
        "    lr=0.001)\n",
        "\n",
        "out_pred_len = modelwrapper.get_output_profile_len()\n",
        "print(out_pred_len, seq_len-out_pred_len)\n",
        "\n",
        "# the code below is used to prepare instances of keras.utils.Sequence that\n",
        "# can be supplied to model.fit_generator(...)\n",
        "# Note that we log-transform our counts using np.log(counts+1)\n",
        "# Also note that the profiles for the control are smoothed by windows of\n",
        "# size 1 and 50 (smoothing by a window of size 1 just returns\n",
        "# the original profile)\n",
        "\n",
        "inputs_coordstovals = coordstovals.core.CoordsToValsJoiner(\n",
        "    coordstovals_list=[\n",
        "      coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
        "        genome_fasta_path=\"mm10_no_alt_analysis_set_ENCODE.fasta\",\n",
        "        mode_name=\"sequence\",\n",
        "        center_size_to_use=seq_len),\n",
        "      coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
        "        counts_mode_name=\"CHIPNexus.logcount\",\n",
        "        profile_mode_name=\"CHIPNexus.profile\",\n",
        "        pos_strand_bigwig_path=\"patchcap/counts.pos.bw\",\n",
        "        neg_strand_bigwig_path=\"patchcap/counts.neg.bw\",\n",
        "        center_size_to_use=out_pred_len),\n",
        "    ]\n",
        ")\n",
        "    \n",
        "\n",
        "targets_coordstovals = CoordsToValsJoiner(\n",
        "    coordstovals_list=[\n",
        "      coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
        "        counts_mode_name=\"CHIPNexus.NANOG.logcount\",\n",
        "        profile_mode_name=\"CHIPNexus.NANOG.profile\",\n",
        "        pos_strand_bigwig_path=\"Nanog/counts.pos.bw\",\n",
        "        neg_strand_bigwig_path=\"Nanog/counts.neg.bw\",\n",
        "        center_size_to_use=out_pred_len),\n",
        "      coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
        "        counts_mode_name=\"CHIPNexus.OCT4.logcount\",\n",
        "        profile_mode_name=\"CHIPNexus.OCT4.profile\",\n",
        "        pos_strand_bigwig_path=\"Oct4/counts.pos.bw\",\n",
        "        neg_strand_bigwig_path=\"Oct4/counts.neg.bw\",\n",
        "        center_size_to_use=out_pred_len),\n",
        "      coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
        "        counts_mode_name=\"CHIPNexus.KLF4.logcount\",\n",
        "        profile_mode_name=\"CHIPNexus.KLF4.profile\",\n",
        "        pos_strand_bigwig_path=\"Klf4/counts.pos.bw\",\n",
        "        neg_strand_bigwig_path=\"Klf4/counts.neg.bw\",\n",
        "        center_size_to_use=out_pred_len),\n",
        "      coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
        "        counts_mode_name=\"CHIPNexus.SOX2.logcount\",\n",
        "        profile_mode_name=\"CHIPNexus.SOX2.profile\",\n",
        "        pos_strand_bigwig_path=\"Sox2/counts.pos.bw\",\n",
        "        neg_strand_bigwig_path=\"Sox2/counts.neg.bw\",\n",
        "        center_size_to_use=out_pred_len),\n",
        "    ]\n",
        ")\n",
        "\n",
        "keras_train_batch_generator = KerasBatchGenerator(\n",
        "  coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
        "      bed_file=\"train_1k_around_summits.bed.gz\",\n",
        "      batch_size=64,\n",
        "      shuffle_before_epoch=True, \n",
        "      seed=1234),\n",
        "  inputs_coordstovals=inputs_coordstovals,\n",
        "  targets_coordstovals=targets_coordstovals,\n",
        "  coordsbatch_transformer=\n",
        "          coordbatchtransformers.ReverseComplementAugmenter().chain(\n",
        "          coordbatchtransformers.UniformJitter(\n",
        "              maxshift=200, chromsizes_file=\"mm10.chrom.sizes\")),\n",
        ")\n",
        "\n",
        "keras_valid_batch_generator = KerasBatchGenerator(\n",
        "  coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
        "            bed_file=\"valid_1k_around_summits.bed.gz\",\n",
        "            batch_size=64,\n",
        "            shuffle_before_epoch=False, \n",
        "            seed=1234),\n",
        "  inputs_coordstovals=inputs_coordstovals,\n",
        "  targets_coordstovals=targets_coordstovals\n",
        ")\n",
        "\n",
        "\n",
        "#As a sanity check, print out the dimensions of everything in individual batches\n",
        "sampinputs,samptargets = keras_train_batch_generator[0]\n",
        "# for key in sampinputs:\n",
        "#   print(key, sampinputs[key].shape)\n",
        "for key in samptargets:\n",
        "  print(key, samptargets[key].shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 346\n",
            "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
            "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
            "CHIPNexus.NANOG.logcount (128, 2)\n",
            "CHIPNexus.NANOG.profile (128, 1000, 2)\n",
            "CHIPNexus.OCT4.logcount (128, 2)\n",
            "CHIPNexus.OCT4.profile (128, 1000, 2)\n",
            "CHIPNexus.KLF4.logcount (128, 2)\n",
            "CHIPNexus.KLF4.profile (128, 1000, 2)\n",
            "CHIPNexus.SOX2.logcount (128, 2)\n",
            "CHIPNexus.SOX2.profile (128, 1000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t7C0ju9WgKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb294ad6-9d37-4252-d102-1d43e97e9d0b"
      },
      "source": [
        "model = modelwrapper.get_keras_model()\n",
        "print(model.summary())\n",
        "\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                            patience=10, restore_best_weights=True)\n",
        "loss_history = model.fit_generator(keras_train_batch_generator,\n",
        "                    epochs=200,\n",
        "                    validation_data=keras_valid_batch_generator,\n",
        "                    callbacks=[early_stopping_callback],\n",
        "                    workers=10)\n",
        "model.set_weights(early_stopping_callback.best_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "sequence (InputLayer)           (None, 1346, 4)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 1326, 64)     5440        sequence[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 1322, 64)     0           conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 1322, 64)     12352       conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 1322, 64)     0           lambda_13[0][0]                  \n",
            "                                                                 conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 1314, 64)     0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 1314, 64)     12352       add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 1314, 64)     0           lambda_14[0][0]                  \n",
            "                                                                 conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 1298, 64)     0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 1298, 64)     12352       add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 1298, 64)     0           lambda_15[0][0]                  \n",
            "                                                                 conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 1266, 64)     0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 1266, 64)     12352       add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 1266, 64)     0           lambda_16[0][0]                  \n",
            "                                                                 conv1d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 1202, 64)     0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_28 (Conv1D)              (None, 1202, 64)     12352       add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 1202, 64)     0           lambda_17[0][0]                  \n",
            "                                                                 conv1d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 1074, 64)     0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_29 (Conv1D)              (None, 1074, 64)     12352       add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 1074, 64)     0           lambda_18[0][0]                  \n",
            "                                                                 conv1d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_3 (Glo (None, 64)           0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.logcount (InputLayer) (None, 2)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_30 (Conv1D)              (None, 1000, 2)      9602        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.profile (InputLayer)  (None, 1000, 2)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_31 (Conv1D)              (None, 1000, 2)      9602        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_32 (Conv1D)              (None, 1000, 2)      9602        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, 1000, 2)      9602        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 66)           0           global_average_pooling1d_3[0][0] \n",
            "                                                                 CHIPNexus.logcount[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 66)           0           global_average_pooling1d_3[0][0] \n",
            "                                                                 CHIPNexus.logcount[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 66)           0           global_average_pooling1d_3[0][0] \n",
            "                                                                 CHIPNexus.logcount[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 66)           0           global_average_pooling1d_3[0][0] \n",
            "                                                                 CHIPNexus.logcount[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 1000, 4)      0           conv1d_30[0][0]                  \n",
            "                                                                 CHIPNexus.profile[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 1000, 4)      0           conv1d_31[0][0]                  \n",
            "                                                                 CHIPNexus.profile[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 1000, 4)      0           conv1d_32[0][0]                  \n",
            "                                                                 CHIPNexus.profile[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 1000, 4)      0           conv1d_33[0][0]                  \n",
            "                                                                 CHIPNexus.profile[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.NANOG.logcount (Dense (None, 2)            134         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.OCT4.logcount (Dense) (None, 2)            134         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.KLF4.logcount (Dense) (None, 2)            134         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.SOX2.logcount (Dense) (None, 2)            134         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.NANOG.profile (Conv1D (None, 1000, 2)      10          concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.OCT4.profile (Conv1D) (None, 1000, 2)      10          concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.KLF4.profile (Conv1D) (None, 1000, 2)      10          concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CHIPNexus.SOX2.profile (Conv1D) (None, 1000, 2)      10          concatenate_24[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 118,536\n",
            "Trainable params: 118,536\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            " 418/2052 [=====>........................] - ETA: 17:46 - loss: 4394.3511 - CHIPNexus.NANOG.logcount_loss: 1.2187 - CHIPNexus.OCT4.logcount_loss: 1.0607 - CHIPNexus.KLF4.logcount_loss: 1.2286 - CHIPNexus.SOX2.logcount_loss: 0.5567 - CHIPNexus.NANOG.profile_loss: 1710.5813 - CHIPNexus.OCT4.profile_loss: 1302.2714 - CHIPNexus.KLF4.profile_loss: 1068.6934 - CHIPNexus.SOX2.profile_loss: 642.3687"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RobIdObuZjJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}