{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abhi CTCF Replication.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvantiShri/colab_notebooks/blob/master/Abhi_CTCF_Replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7qPI3HE2rXkS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division, print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9HfwzxNrdLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2949
        },
        "outputId": "5f23d16b-3209-4ba6-cb13-288dbc9c6773"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyfasta\n",
        "!pip install deeptools\n",
        "!pip install momma_dragonn\n",
        "\n",
        "!apt-get install bedtools\n",
        "\n",
        "#Download optimal IDR thresholded peaks\n",
        "![[ -f idr_optimal_file.narrowPeak.gz ]] || wget https://www.encodeproject.org/files/ENCFF002CEL/@@download/ENCFF002CEL.bed.gz -O idr_optimal_file.narrowPeak.gz\n",
        "  \n",
        "#Get hg19 fasta by download 2bit and then converting to fa\n",
        "![[ -f hg19.2bit ]] || wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.2bit -O hg19.2bit  \n",
        "![[ -f twoBitToFa ]] || wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa -O twoBitToFa\n",
        "!chmod a+x twoBitToFa\n",
        "![[ -f hg19.genome.fa ]] || ./twoBitToFa hg19.2bit hg19.genome.fa\n",
        "  \n",
        "#download hg19 chromsizes file\n",
        "![[ -f hg19.chrom.sizes ]] || wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.chrom.sizes -O hg19.chrom.sizes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyfasta\n",
            "  Downloading https://files.pythonhosted.org/packages/be/3f/794fbcdaaa2113f0a1d16a962463896c1a6bdab77bd63f33a8f16aae6cdc/pyfasta-0.5.2.tar.gz\n",
            "Building wheels for collected packages: pyfasta\n",
            "  Running setup.py bdist_wheel for pyfasta ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ef/57/33/7b569168243dfbfe60bd3533e6897d170b391f2ce58df9d275\n",
            "Successfully built pyfasta\n",
            "Installing collected packages: pyfasta\n",
            "Successfully installed pyfasta-0.5.2\n",
            "Collecting deeptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/0b/0ff8d6440feba7fa4e7911ab0d13bab4ce5c72c9be2ffa6633f417a7cfef/deepTools-3.1.3.tar.gz (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from deeptools) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from deeptools) (1.1.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from deeptools) (2.1.2)\n",
            "Collecting pysam>=0.14.0 (from deeptools)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/f4/e12faf1618e977a868fe66d40288fc3dd989a10cc4c77603da33a7634a18/pysam-0.15.1-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 9.7MB 1.1MB/s \n",
            "\u001b[?25hCollecting numpydoc>=0.5 (from deeptools)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n",
            "Collecting pyBigWig>=0.2.1 (from deeptools)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/0d/8e2a1edb9524790c6a4d0b70bc800a8e4afee1bc7bdd048c54b8d9cf1c32/pyBigWig-0.3.12.tar.gz (66kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 22.6MB/s \n",
            "\u001b[?25hCollecting py2bit>=0.2.0 (from deeptools)\n",
            "  Downloading https://files.pythonhosted.org/packages/53/bb/547a927bed736ead3dc909e1e552d57c9034bb9493eff80544c0cf6e4828/py2bit-0.3.0.tar.gz\n",
            "Collecting plotly>=2.0.0 (from deeptools)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/d6/82333db5a6f56f828d817d49ae6ea153125d70214a189686afe784e159ad/plotly-3.4.2-py2.py3-none-any.whl (37.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.8MB 842kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->deeptools) (2018.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->deeptools) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->deeptools) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->deeptools) (1.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->deeptools) (2.3.0)\n",
            "Collecting sphinx>=1.2.3 (from numpydoc>=0.5->deeptools)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/d5/3a8727d6f890b1ae45da72a55bf8449e9f2c535a444923b338c3f509f203/Sphinx-1.8.2-py2.py3-none-any.whl (3.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.1MB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.5->deeptools) (2.10)\n",
            "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/dist-packages (from plotly>=2.0.0->deeptools) (4.4.0)\n",
            "Collecting retrying>=1.3.3 (from plotly>=2.0.0->deeptools)\n",
            "  Downloading https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz\n",
            "Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/dist-packages (from plotly>=2.0.0->deeptools) (4.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from plotly>=2.0.0->deeptools) (2.18.4)\n",
            "Collecting babel!=2.0,>=1.3 (from sphinx>=1.2.3->numpydoc>=0.5->deeptools)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/ad/c6f60602d3ee3d92fbed87675b6fb6a6f9a38c223343ababdb44ba201f10/Babel-2.6.0-py2.py3-none-any.whl (8.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.1MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.5->deeptools) (40.6.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.5->deeptools) (2.1.3)\n",
            "Collecting packaging (from sphinx>=1.2.3->numpydoc>=0.5->deeptools)\n",
            "  Downloading https://files.pythonhosted.org/packages/89/d1/92e6df2e503a69df9faab187c684585f0136662c12bb1f36901d426f3fab/packaging-18.0-py2.py3-none-any.whl\n",
            "Collecting sphinxcontrib-websupport (from sphinx>=1.2.3->numpydoc>=0.5->deeptools)\n",
            "  Downloading https://files.pythonhosted.org/packages/52/69/3c2fbdc3702358c5b34ee25e387b24838597ef099761fc9a42c166796e8f/sphinxcontrib_websupport-1.1.0-py2.py3-none-any.whl\n",
            "Collecting alabaster<0.8,>=0.7 (from sphinx>=1.2.3->numpydoc>=0.5->deeptools)\n",
            "  Downloading https://files.pythonhosted.org/packages/10/ad/00b090d23a222943eb0eda509720a404f531a439e803f6538f35136cae9e/alabaster-0.7.12-py2.py3-none-any.whl\n",
            "Collecting imagesize (from sphinx>=1.2.3->numpydoc>=0.5->deeptools)\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/b6/aef66b4c52a6ad6ac18cf6ebc5731ed06d8c9ae4d3b2d9951f261150be67/imagesize-1.1.0-py2.py3-none-any.whl\n",
            "Collecting docutils>=0.11 (from sphinx>=1.2.3->numpydoc>=0.5->deeptools)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 21.1MB/s \n",
            "\u001b[?25hCollecting snowballstemmer>=1.1 (from sphinx>=1.2.3->numpydoc>=0.5->deeptools)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/6c/8a935e2c7b54a37714656d753e4187ee0631988184ed50c0cf6476858566/snowballstemmer-1.2.1-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.5->deeptools) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly>=2.0.0->deeptools) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly>=2.0.0->deeptools) (4.3.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly>=2.0.0->deeptools) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly>=2.0.0->deeptools) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->plotly>=2.0.0->deeptools) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->plotly>=2.0.0->deeptools) (2018.10.15)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->plotly>=2.0.0->deeptools) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->plotly>=2.0.0->deeptools) (3.0.4)\n",
            "Building wheels for collected packages: deeptools, numpydoc, pyBigWig, py2bit, retrying\n",
            "  Running setup.py bdist_wheel for deeptools ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/26/7b/1a/e828c248cbb05db00a41118e24c0435bcdbf1b750f4dd2c0c8\n",
            "  Running setup.py bdist_wheel for numpydoc ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n",
            "  Running setup.py bdist_wheel for pyBigWig ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8c/9d/da/696632dbd03ef4f9f1988851a411dad15781ac8aabffce49fc\n",
            "  Running setup.py bdist_wheel for py2bit ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/df/66/b6/33fb9b65b31121127f1da60ca27948ecf8d4c59b0967298de8\n",
            "  Running setup.py bdist_wheel for retrying ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c\n",
            "Successfully built deeptools numpydoc pyBigWig py2bit retrying\n",
            "Installing collected packages: pysam, babel, packaging, sphinxcontrib-websupport, alabaster, imagesize, docutils, snowballstemmer, sphinx, numpydoc, pyBigWig, py2bit, retrying, plotly, deeptools\n",
            "  Found existing installation: plotly 1.12.12\n",
            "    Uninstalling plotly-1.12.12:\n",
            "      Successfully uninstalled plotly-1.12.12\n",
            "Successfully installed alabaster-0.7.12 babel-2.6.0 deeptools-3.1.3 docutils-0.14 imagesize-1.1.0 numpydoc-0.8.0 packaging-18.0 plotly-3.4.2 py2bit-0.3.0 pyBigWig-0.3.12 pysam-0.15.1 retrying-1.3.3 snowballstemmer-1.2.1 sphinx-1.8.2 sphinxcontrib-websupport-1.1.0\n",
            "Collecting momma_dragonn\n",
            "  Downloading https://files.pythonhosted.org/packages/86/5f/919f8a01ebec00ddeb4b2432b74bc2f6c6077a376bd0936983586a5fe9eb/momma_dragonn-0.2.7.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from momma_dragonn) (1.14.6)\n",
            "Collecting avutils (from momma_dragonn)\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/a1/f99f27a5065c2daae0f9711705ba4fc7bd777f160292dcae14bb6e5e9838/avutils-0.2.2.0.tar.gz\n",
            "Building wheels for collected packages: momma-dragonn, avutils\n",
            "  Running setup.py bdist_wheel for momma-dragonn ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/87/e8/25/a7325bcfdc08dc594687cc1cda8915c217c602fa5586e4835f\n",
            "  Running setup.py bdist_wheel for avutils ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/10/60/224c0029355b3627d218534df8cbf15588409cdd6bc8df0993\n",
            "Successfully built momma-dragonn avutils\n",
            "Installing collected packages: avutils, momma-dragonn\n",
            "Successfully installed avutils-0.2.2.0 momma-dragonn-0.2.7.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  bedtools\n",
            "0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 577 kB of archives.\n",
            "After this operation, 2,040 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 bedtools amd64 2.26.0+dfsg-5 [577 kB]\n",
            "Fetched 577 kB in 2s (283 kB/s)\n",
            "Selecting previously unselected package bedtools.\n",
            "(Reading database ... 22298 files and directories currently installed.)\n",
            "Preparing to unpack .../bedtools_2.26.0+dfsg-5_amd64.deb ...\n",
            "Unpacking bedtools (2.26.0+dfsg-5) ...\n",
            "Setting up bedtools (2.26.0+dfsg-5) ...\n",
            "--2018-11-28 07:35:01--  https://www.encodeproject.org/files/ENCFF002CEL/@@download/ENCFF002CEL.bed.gz\n",
            "Resolving www.encodeproject.org (www.encodeproject.org)... 171.67.205.70\n",
            "Connecting to www.encodeproject.org (www.encodeproject.org)|171.67.205.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://download.encodeproject.org/https://encode-files.s3.amazonaws.com/2014/06/06/23bfd2ac-7a0d-46b2-b3bf-a34f6d03134f/ENCFF002CEL.narrowPeak.gz?response-content-disposition=attachment%3B%20filename%3DENCFF002CEL.bed.gz&x-amz-security-token=FQoGZXIvYXdzEEAaDPz3btDGTOXkv6veJCK3A%2BZ2c9aezwvNNneV1CCo9hhpS6j%2BsQy8Mq2a8frxcLjmBR0%2FdBI0UwxZDl2dXZqUCQnaCK7ihNK5K2WEwQaS96%2FKMLfe0f28g%2F46aKJk6XpHcPff3XLJdAQOdi9fyUrMbookQN4gTKyf41pqE5IgempDK1t7UMkpNuSin2W5lPEHxS5a1PB64Zv1zZM%2FBWHma%2Fln3reBaz1qpha2dFHnwnYswsArr%2FECMmcJubnxQl9T3h8CPHnmbL1MOqkHdpemIOokxNmHN7jcn5Z2%2Femvm5OrwIl6LJ76or2la9mHz1fvIk7%2B2FOFO2CY5nKwsc4eOAq%2FagqLfVpdAsqlRpmClvvZcCXgAyTEdv9C6EyXYp%2FcVp0N4fQKp%2B%2BCxSPxbFXf8ktNMbzkiAA%2BgJYqwOHfOuVBjg3IshqZrjGvhwSRgAbfDg%2F1won7zFUOnCcgTAcW4TfMg89XuONp5ch8qFkz13sY3UAMvOmMxtEZT2x4amqVZxTHBDoacjIpSmno333n1HtzAi4kL82oVuOqELlPmrZxua5m9hy0YtjEtRtzQqLzlAAc%2BfhAwGPKHEimcL6OjFDWNXOPKfEo8OH43wU%3D&Expires=1543520102&AWSAccessKeyId=ASIATGZNGCNX64WMVSWC&Signature=iRoU2kao95v0wM3ck53nPXNo4mQ%3D [following]\n",
            "--2018-11-28 07:35:02--  https://download.encodeproject.org/https://encode-files.s3.amazonaws.com/2014/06/06/23bfd2ac-7a0d-46b2-b3bf-a34f6d03134f/ENCFF002CEL.narrowPeak.gz?response-content-disposition=attachment%3B%20filename%3DENCFF002CEL.bed.gz&x-amz-security-token=FQoGZXIvYXdzEEAaDPz3btDGTOXkv6veJCK3A%2BZ2c9aezwvNNneV1CCo9hhpS6j%2BsQy8Mq2a8frxcLjmBR0%2FdBI0UwxZDl2dXZqUCQnaCK7ihNK5K2WEwQaS96%2FKMLfe0f28g%2F46aKJk6XpHcPff3XLJdAQOdi9fyUrMbookQN4gTKyf41pqE5IgempDK1t7UMkpNuSin2W5lPEHxS5a1PB64Zv1zZM%2FBWHma%2Fln3reBaz1qpha2dFHnwnYswsArr%2FECMmcJubnxQl9T3h8CPHnmbL1MOqkHdpemIOokxNmHN7jcn5Z2%2Femvm5OrwIl6LJ76or2la9mHz1fvIk7%2B2FOFO2CY5nKwsc4eOAq%2FagqLfVpdAsqlRpmClvvZcCXgAyTEdv9C6EyXYp%2FcVp0N4fQKp%2B%2BCxSPxbFXf8ktNMbzkiAA%2BgJYqwOHfOuVBjg3IshqZrjGvhwSRgAbfDg%2F1won7zFUOnCcgTAcW4TfMg89XuONp5ch8qFkz13sY3UAMvOmMxtEZT2x4amqVZxTHBDoacjIpSmno333n1HtzAi4kL82oVuOqELlPmrZxua5m9hy0YtjEtRtzQqLzlAAc%2BfhAwGPKHEimcL6OjFDWNXOPKfEo8OH43wU%3D&Expires=1543520102&AWSAccessKeyId=ASIATGZNGCNX64WMVSWC&Signature=iRoU2kao95v0wM3ck53nPXNo4mQ%3D\n",
            "Resolving download.encodeproject.org (download.encodeproject.org)... 171.67.205.70\n",
            "Connecting to download.encodeproject.org (download.encodeproject.org)|171.67.205.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://encode-files.s3.amazonaws.com/2014/06/06/23bfd2ac-7a0d-46b2-b3bf-a34f6d03134f/ENCFF002CEL.narrowPeak.gz?response-content-disposition=attachment%3B%20filename%3DENCFF002CEL.bed.gz&x-amz-security-token=FQoGZXIvYXdzEEAaDPz3btDGTOXkv6veJCK3A%2BZ2c9aezwvNNneV1CCo9hhpS6j%2BsQy8Mq2a8frxcLjmBR0%2FdBI0UwxZDl2dXZqUCQnaCK7ihNK5K2WEwQaS96%2FKMLfe0f28g%2F46aKJk6XpHcPff3XLJdAQOdi9fyUrMbookQN4gTKyf41pqE5IgempDK1t7UMkpNuSin2W5lPEHxS5a1PB64Zv1zZM%2FBWHma%2Fln3reBaz1qpha2dFHnwnYswsArr%2FECMmcJubnxQl9T3h8CPHnmbL1MOqkHdpemIOokxNmHN7jcn5Z2%2Femvm5OrwIl6LJ76or2la9mHz1fvIk7%2B2FOFO2CY5nKwsc4eOAq%2FagqLfVpdAsqlRpmClvvZcCXgAyTEdv9C6EyXYp%2FcVp0N4fQKp%2B%2BCxSPxbFXf8ktNMbzkiAA%2BgJYqwOHfOuVBjg3IshqZrjGvhwSRgAbfDg%2F1won7zFUOnCcgTAcW4TfMg89XuONp5ch8qFkz13sY3UAMvOmMxtEZT2x4amqVZxTHBDoacjIpSmno333n1HtzAi4kL82oVuOqELlPmrZxua5m9hy0YtjEtRtzQqLzlAAc%2BfhAwGPKHEimcL6OjFDWNXOPKfEo8OH43wU%3D&Expires=1543520102&AWSAccessKeyId=ASIATGZNGCNX64WMVSWC&Signature=iRoU2kao95v0wM3ck53nPXNo4mQ%3D [following]\n",
            "--2018-11-28 07:35:03--  https://encode-files.s3.amazonaws.com/2014/06/06/23bfd2ac-7a0d-46b2-b3bf-a34f6d03134f/ENCFF002CEL.narrowPeak.gz?response-content-disposition=attachment%3B%20filename%3DENCFF002CEL.bed.gz&x-amz-security-token=FQoGZXIvYXdzEEAaDPz3btDGTOXkv6veJCK3A%2BZ2c9aezwvNNneV1CCo9hhpS6j%2BsQy8Mq2a8frxcLjmBR0%2FdBI0UwxZDl2dXZqUCQnaCK7ihNK5K2WEwQaS96%2FKMLfe0f28g%2F46aKJk6XpHcPff3XLJdAQOdi9fyUrMbookQN4gTKyf41pqE5IgempDK1t7UMkpNuSin2W5lPEHxS5a1PB64Zv1zZM%2FBWHma%2Fln3reBaz1qpha2dFHnwnYswsArr%2FECMmcJubnxQl9T3h8CPHnmbL1MOqkHdpemIOokxNmHN7jcn5Z2%2Femvm5OrwIl6LJ76or2la9mHz1fvIk7%2B2FOFO2CY5nKwsc4eOAq%2FagqLfVpdAsqlRpmClvvZcCXgAyTEdv9C6EyXYp%2FcVp0N4fQKp%2B%2BCxSPxbFXf8ktNMbzkiAA%2BgJYqwOHfOuVBjg3IshqZrjGvhwSRgAbfDg%2F1won7zFUOnCcgTAcW4TfMg89XuONp5ch8qFkz13sY3UAMvOmMxtEZT2x4amqVZxTHBDoacjIpSmno333n1HtzAi4kL82oVuOqELlPmrZxua5m9hy0YtjEtRtzQqLzlAAc%2BfhAwGPKHEimcL6OjFDWNXOPKfEo8OH43wU%3D&Expires=1543520102&AWSAccessKeyId=ASIATGZNGCNX64WMVSWC&Signature=iRoU2kao95v0wM3ck53nPXNo4mQ%3D\n",
            "Resolving encode-files.s3.amazonaws.com (encode-files.s3.amazonaws.com)... 52.218.248.202\n",
            "Connecting to encode-files.s3.amazonaws.com (encode-files.s3.amazonaws.com)|52.218.248.202|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1231983 (1.2M) [binary/octet-stream]\n",
            "Saving to: ‘idr_optimal_file.narrowPeak.gz’\n",
            "\n",
            "idr_optimal_file.na 100%[===================>]   1.17M  1.37MB/s    in 0.9s    \n",
            "\n",
            "2018-11-28 07:35:04 (1.37 MB/s) - ‘idr_optimal_file.narrowPeak.gz’ saved [1231983/1231983]\n",
            "\n",
            "--2018-11-28 07:35:06--  http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.2bit\n",
            "Resolving hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 816241703 (778M) [text/plain]\n",
            "Saving to: ‘hg19.2bit’\n",
            "\n",
            "hg19.2bit           100%[===================>] 778.43M  21.1MB/s    in 51s     \n",
            "\n",
            "2018-11-28 07:35:57 (15.4 MB/s) - ‘hg19.2bit’ saved [816241703/816241703]\n",
            "\n",
            "--2018-11-28 07:35:58--  http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/twoBitToFa\n",
            "Resolving hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.soe.ucsc.edu (hgdownload.soe.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5873736 (5.6M) [text/plain]\n",
            "Saving to: ‘twoBitToFa’\n",
            "\n",
            "twoBitToFa          100%[===================>]   5.60M   917KB/s    in 8.2s    \n",
            "\n",
            "2018-11-28 07:36:07 (698 KB/s) - ‘twoBitToFa’ saved [5873736/5873736]\n",
            "\n",
            "--2018-11-28 07:36:31--  http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/hg19.chrom.sizes\n",
            "Resolving hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1971 (1.9K) [text/plain]\n",
            "Saving to: ‘hg19.chrom.sizes’\n",
            "\n",
            "hg19.chrom.sizes    100%[===================>]   1.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-11-28 07:36:31 (189 MB/s) - ‘hg19.chrom.sizes’ saved [1971/1971]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aocJNnHxuVvP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#do an asinh transform of column 7 (SPP singal strength)\n",
        "!zcat idr_optimal_file.narrowPeak.gz | perl -lane 'BEGIN{use Math::Trig} {print $F[0].\"\\t\".(($F[1]+$F[9])).\"\\t\".(($F[1]+$F[9])).\"\\t\".asinh($F[6])}' | bedtools slop -g hg19.chrom.sizes -b 500 | perl -lane 'if ($F[2]-$F[1]==1000) {print $F[0].\"\\t\".$F[1].\"\\t\".$F[2].\"\\t$F[3]\"}' | sortBed | gzip -c > positives_asinh_spp.gz\n",
        "#15% of the regions go to validation and test set. Ensure no overlap between test, valid and train sets using bedtools intersect -v\n",
        "!zcat positives_asinh_spp.gz | perl -lne 'if ($.%15==0) {print $_}' | gzip -c > test_positives_asinh_spp.gz\n",
        "!bedtools intersect -sorted -v -a <(zcat positives_asinh_spp.gz | perl -lne 'if ($.%15==10) {print $_}') -b test_positives_asinh_spp.gz -wa | gzip -c > valid_positives_asinh_spp.gz\n",
        "!bedtools intersect -sorted -v -a positives_asinh_spp.gz -b <(zcat valid_positives_asinh_spp.gz test_positives_asinh_spp.gz | sortBed) -wa | gzip -c > train_positives_asinh_spp.gz\n",
        "\n",
        "#!zcat positives_asinh_spp.gz | egrep -w -v 'chr2|chr21' | gzip -c > train_positives_asinh_spp.gz\n",
        "#!zcat positives_asinh_spp.gz | egrep -w 'chr21' | gzip -c > valid_positives_asinh_spp.gz\n",
        "#!zcat positives_asinh_spp.gz | egrep -w 'chr2' | gzip -c > test_positives_asinh_spp.gz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vd24tVyYr8y4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from momma_dragonn.data_loaders import pyfasta_data_loader\n",
        "import six\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_single_stream_data_generator(bed_source, batch_size):\n",
        "\n",
        "  return pyfasta_data_loader.SingleStreamSeqOnly(\n",
        "               batch_size=batch_size,\n",
        "               bed_source=bed_source,\n",
        "               fasta_data_source=\"hg19.genome.fa\",\n",
        "               rc_augment=True,\n",
        "               num_to_load_for_eval=100, #not used\n",
        "               labels_dtype=\"float\",\n",
        "               wrap_in_keys=[\"sequence\", \"output\"],\n",
        "               randomize_after_pass=True)\n",
        "\n",
        "def train_regression_model(num_outputs,\n",
        "                           train_bed_source,\n",
        "                           valid_bed_source,\n",
        "                           steps_per_epoch=200,\n",
        "                           batch_size=128,\n",
        "                           adam_lr=0.001,\n",
        "                           num_validation_samples=3500,\n",
        "                           num_conv_filters=15,\n",
        "                           conv_filter_length=15,\n",
        "                           pool_length_and_stride=5,\n",
        "                           num_dense_units=[100],\n",
        "                           preinit_model=None,\n",
        "                           epochs_to_train_for=60):\n",
        "  \n",
        "  validation_steps = int(num_validation_samples/batch_size)\n",
        "  \n",
        "  \n",
        "  class SpearmanCorrCallback(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, validation_steps, valid_data_generator):\n",
        "      self.validation_steps = validation_steps\n",
        "      self.valid_data_generator = valid_data_generator\n",
        "  \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "      valid_data_preds = []\n",
        "      valid_data_labels = []\n",
        "      for batch_num in range(self.validation_steps):\n",
        "        (X_batch, y_batch) = six.next(self.valid_data_generator)\n",
        "        valid_data_labels.extend(y_batch[\"output\"])\n",
        "        valid_data_preds.extend(self.model.predict(X_batch))\n",
        "      valid_data_preds = np.array(valid_data_preds)\n",
        "      valid_data_labels = np.array(valid_data_labels)\n",
        "      \n",
        "      print(spearmanr(a=valid_data_labels[:,0],\n",
        "                      b=valid_data_preds[:,0]))\n",
        "      \n",
        "  \n",
        "  if (preinit_model is None):\n",
        "    input = keras.layers.Input(shape=(1000,4), name=\"sequence\")\n",
        "    conv1 = keras.layers.convolutional.Conv1D(\n",
        "              filters=num_conv_filters, kernel_size=conv_filter_length,\n",
        "              padding=\"same\")(input)\n",
        "    conv1relu = keras.layers.core.Activation(activation=\"relu\")(conv1)\n",
        "    conv2 = keras.layers.convolutional.Conv1D(\n",
        "              filters=num_conv_filters,\n",
        "              kernel_size=conv_filter_length,\n",
        "              padding=\"same\")(conv1relu)\n",
        "    conv2relu = keras.layers.core.Activation(activation=\"relu\")(conv2)\n",
        "    conv3 = keras.layers.convolutional.Conv1D(\n",
        "              filters=num_conv_filters,\n",
        "              kernel_size=conv_filter_length,\n",
        "              padding=\"same\")(conv2relu)\n",
        "    conv3relu = keras.layers.core.Activation(activation=\"relu\")(conv3)\n",
        "    avgpool = keras.layers.convolutional.AveragePooling1D(\n",
        "                pool_size=pool_length_and_stride,\n",
        "                strides=pool_length_and_stride)(conv3relu)\n",
        "    flatten = keras.layers.core.Flatten()(avgpool)\n",
        "    dense1 = keras.layers.core.Dense(units=num_dense_units[0])(flatten)\n",
        "    dense1relu = keras.layers.core.Activation(activation=\"relu\")(dense1)\n",
        "    output = keras.layers.core.Dense(units=num_outputs,\n",
        "                                              name=\"output\")(dense1relu)\n",
        "    model = Model(inputs=[input], outputs=[output])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=adam_lr),\n",
        "                  loss={\"output\": \"mse\"})\n",
        "  else:\n",
        "    model = preinit_model\n",
        "  \n",
        "  model.summary()\n",
        "  early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                              monitor='val_loss',\n",
        "                              patience=epochs_to_train_for,\n",
        "                              restore_best_weights=True)\n",
        "  fit_history = model.fit_generator(\n",
        "    generator=get_single_stream_data_generator(\n",
        "      bed_source=train_bed_source,\n",
        "      batch_size=batch_size).get_batch_generator(),\n",
        "    validation_data= get_single_stream_data_generator(\n",
        "      bed_source=valid_bed_source,\n",
        "      batch_size=batch_size).get_batch_generator(),\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=epochs_to_train_for,\n",
        "    callbacks=[early_stopping_callback,\n",
        "               #Need to have two separate generators for the callback and\n",
        "               # for the main training method to keep keras happy\n",
        "               SpearmanCorrCallback(\n",
        "                   valid_data_generator=get_single_stream_data_generator(\n",
        "                      bed_source=valid_bed_source,\n",
        "                      batch_size=batch_size).get_batch_generator(),\n",
        "                   validation_steps=validation_steps)]\n",
        "  )\n",
        "  #the callback isn't triggered if the upper epoch limit is hit,\n",
        "  # so make sure the set the weights from the best epoch at the end\n",
        "  model.set_weights(early_stopping_callback.best_weights)\n",
        "  \n",
        "  return model, fit_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mke26Z5BuS4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3778
        },
        "outputId": "e481ea88-b4ea-4228-a61d-d068409d77fa"
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "positives_asinh_spp_model, positives_asinh_spp_fit_history = train_regression_model(num_outputs=1,\n",
        "                             train_bed_source=\"train_positives_asinh_spp.gz\",\n",
        "                             valid_bed_source=\"valid_positives_asinh_spp.gz\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequence (InputLayer)        (None, 1000, 4)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 1000, 15)          915       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1000, 15)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 1000, 15)          3390      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 1000, 15)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 1000, 15)          3390      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1000, 15)          0         \n",
            "_________________________________________________________________\n",
            "average_pooling1d_2 (Average (None, 200, 15)           0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               300100    \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 307,896\n",
            "Trainable params: 307,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "Reading bed file valid_positives_asinh_spp.gz into memory\n",
            "Reading bed file train_positives_asinh_spp.gz into memory\n",
            "Finished reading bed file into memory; got 3466rows\n",
            "Finished reading bed file into memory; got 44031rows\n",
            "200/200 [==============================] - 31s 154ms/step - loss: 1.2062 - val_loss: 0.7407\n",
            "Reading bed file valid_positives_asinh_spp.gz into memory\n",
            "Finished reading bed file into memory; got 3466rows\n",
            "SpearmanrResult(correlation=0.09307422488728136, pvalue=8.940730887047221e-15)\n",
            "Epoch 2/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.7240 - val_loss: 0.7017\n",
            "SpearmanrResult(correlation=0.25298674571811375, pvalue=2.1505729521287494e-101)\n",
            "Epoch 3/60\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 0.6561 - val_loss: 0.6429\n",
            "SpearmanrResult(correlation=0.3624452669264781, pvalue=1.2669799508812112e-213)\n",
            "Epoch 4/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.5915 - val_loss: 0.6157\n",
            "SpearmanrResult(correlation=0.4542597093068603, pvalue=0.0)\n",
            "Epoch 5/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.5487 - val_loss: 0.5676\n",
            "SpearmanrResult(correlation=0.4937260641623749, pvalue=0.0)\n",
            "Epoch 6/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.5168 - val_loss: 0.5430\n",
            "SpearmanrResult(correlation=0.5297255471582037, pvalue=0.0)\n",
            "Epoch 7/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.5049 - val_loss: 0.5157\n",
            "SpearmanrResult(correlation=0.5444556075142446, pvalue=0.0)\n",
            "Epoch 8/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.4846 - val_loss: 0.5314\n",
            "SpearmanrResult(correlation=0.5656484266545518, pvalue=0.0)\n",
            "Epoch 9/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.4787 - val_loss: 0.5050\n",
            "SpearmanrResult(correlation=0.5698180615610607, pvalue=0.0)\n",
            "Epoch 10/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.4764 - val_loss: 0.5371\n",
            "SpearmanrResult(correlation=0.5886948250287103, pvalue=0.0)\n",
            "Epoch 11/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.4664 - val_loss: 0.6343\n",
            "SpearmanrResult(correlation=0.5938129614758715, pvalue=0.0)\n",
            "Epoch 12/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.4621 - val_loss: 0.4879\n",
            "SpearmanrResult(correlation=0.5987493631125328, pvalue=0.0)\n",
            "Epoch 13/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.4438 - val_loss: 0.4848\n",
            "SpearmanrResult(correlation=0.6064330143695986, pvalue=0.0)\n",
            "Epoch 14/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.4400 - val_loss: 0.4558\n",
            "SpearmanrResult(correlation=0.608948039324487, pvalue=0.0)\n",
            "Epoch 15/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.4326 - val_loss: 0.4573\n",
            "SpearmanrResult(correlation=0.6173362569555508, pvalue=0.0)\n",
            "Epoch 16/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.4393 - val_loss: 0.4572\n",
            "SpearmanrResult(correlation=0.6175867954675174, pvalue=0.0)\n",
            "Epoch 17/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.4341 - val_loss: 0.4517\n",
            "SpearmanrResult(correlation=0.6187844332405908, pvalue=0.0)\n",
            "Epoch 18/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.4130 - val_loss: 0.4989\n",
            "SpearmanrResult(correlation=0.6224844069016899, pvalue=0.0)\n",
            "Epoch 19/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.4183 - val_loss: 0.4381\n",
            "SpearmanrResult(correlation=0.6306096350729556, pvalue=0.0)\n",
            "Epoch 20/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.4127 - val_loss: 0.4452\n",
            "SpearmanrResult(correlation=0.6323498964202863, pvalue=0.0)\n",
            "Epoch 21/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.4103 - val_loss: 0.4554\n",
            "SpearmanrResult(correlation=0.6283576864010134, pvalue=0.0)\n",
            "Epoch 22/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.4045 - val_loss: 0.4389\n",
            "SpearmanrResult(correlation=0.6315528121678553, pvalue=0.0)\n",
            "Epoch 23/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.4018 - val_loss: 0.4393\n",
            "SpearmanrResult(correlation=0.6332818402889931, pvalue=0.0)\n",
            "Epoch 24/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.3949 - val_loss: 0.4396\n",
            "SpearmanrResult(correlation=0.6399018941569364, pvalue=0.0)\n",
            "Epoch 25/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.4006 - val_loss: 0.4352\n",
            "SpearmanrResult(correlation=0.6353296817073337, pvalue=0.0)\n",
            "Epoch 26/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.3914 - val_loss: 0.4536\n",
            "SpearmanrResult(correlation=0.6409720617585348, pvalue=0.0)\n",
            "Epoch 27/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3939 - val_loss: 0.4719\n",
            "SpearmanrResult(correlation=0.6266873792202949, pvalue=0.0)\n",
            "Epoch 28/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.3880 - val_loss: 0.4323\n",
            "SpearmanrResult(correlation=0.6367946694730123, pvalue=0.0)\n",
            "Epoch 29/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.3841 - val_loss: 0.4339\n",
            "SpearmanrResult(correlation=0.6355865245597279, pvalue=0.0)\n",
            "Epoch 30/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.3788 - val_loss: 0.4331\n",
            "SpearmanrResult(correlation=0.6362251427925525, pvalue=0.0)\n",
            "Epoch 31/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.3842 - val_loss: 0.4456\n",
            "SpearmanrResult(correlation=0.6375430633407623, pvalue=0.0)\n",
            "Epoch 32/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.3754 - val_loss: 0.4563\n",
            "SpearmanrResult(correlation=0.6335456422478054, pvalue=0.0)\n",
            "Epoch 33/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3795 - val_loss: 0.4286\n",
            "SpearmanrResult(correlation=0.6328052413763957, pvalue=0.0)\n",
            "Epoch 34/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.3724 - val_loss: 0.4461\n",
            "SpearmanrResult(correlation=0.6418042376947888, pvalue=0.0)\n",
            "Epoch 35/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3748 - val_loss: 0.4246\n",
            "SpearmanrResult(correlation=0.6413687016223523, pvalue=0.0)\n",
            "Epoch 36/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.3692 - val_loss: 0.4440\n",
            "SpearmanrResult(correlation=0.6363620655869138, pvalue=0.0)\n",
            "Epoch 37/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3632 - val_loss: 0.4389\n",
            "SpearmanrResult(correlation=0.6315796209819122, pvalue=0.0)\n",
            "Epoch 38/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.3634 - val_loss: 0.4550\n",
            "SpearmanrResult(correlation=0.6352129498400455, pvalue=0.0)\n",
            "Epoch 39/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.3574 - val_loss: 0.4368\n",
            "SpearmanrResult(correlation=0.6412219070796427, pvalue=0.0)\n",
            "Epoch 40/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.3649 - val_loss: 0.4492\n",
            "SpearmanrResult(correlation=0.63045695811885, pvalue=0.0)\n",
            "Epoch 41/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.3578 - val_loss: 0.4326\n",
            "SpearmanrResult(correlation=0.6437486918627546, pvalue=0.0)\n",
            "Epoch 42/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.3511 - val_loss: 0.4740\n",
            "SpearmanrResult(correlation=0.6326502936569851, pvalue=0.0)\n",
            "Epoch 43/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.3605 - val_loss: 0.4289\n",
            "SpearmanrResult(correlation=0.6407356739968156, pvalue=0.0)\n",
            "Epoch 44/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.3542 - val_loss: 0.4558\n",
            "SpearmanrResult(correlation=0.6383981912532007, pvalue=0.0)\n",
            "Epoch 45/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3571 - val_loss: 0.4422\n",
            "SpearmanrResult(correlation=0.6411932777934063, pvalue=0.0)\n",
            "Epoch 46/60\n",
            "200/200 [==============================] - 28s 142ms/step - loss: 0.3507 - val_loss: 0.4480\n",
            "SpearmanrResult(correlation=0.6328727589425238, pvalue=0.0)\n",
            "Epoch 47/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.3513 - val_loss: 0.4438\n",
            "SpearmanrResult(correlation=0.627436416982385, pvalue=0.0)\n",
            "Epoch 48/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.3464 - val_loss: 0.4355\n",
            "SpearmanrResult(correlation=0.6362889762789835, pvalue=0.0)\n",
            "Epoch 49/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3395 - val_loss: 0.4511\n",
            "SpearmanrResult(correlation=0.6319870153918092, pvalue=0.0)\n",
            "Epoch 50/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.3477 - val_loss: 0.4520\n",
            "SpearmanrResult(correlation=0.633581967513255, pvalue=0.0)\n",
            "Epoch 51/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3417 - val_loss: 0.4338\n",
            "SpearmanrResult(correlation=0.6293170895700474, pvalue=0.0)\n",
            "Epoch 52/60\n",
            "200/200 [==============================] - 27s 137ms/step - loss: 0.3385 - val_loss: 0.4365\n",
            "SpearmanrResult(correlation=0.6374982439279724, pvalue=0.0)\n",
            "Epoch 53/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.3367 - val_loss: 0.4506\n",
            "SpearmanrResult(correlation=0.6319581373072396, pvalue=0.0)\n",
            "Epoch 54/60\n",
            "200/200 [==============================] - 28s 138ms/step - loss: 0.3338 - val_loss: 0.4408\n",
            "SpearmanrResult(correlation=0.6324210118325628, pvalue=0.0)\n",
            "Epoch 55/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.3407 - val_loss: 0.4452\n",
            "SpearmanrResult(correlation=0.6303722476615653, pvalue=0.0)\n",
            "Epoch 56/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3255 - val_loss: 0.4414\n",
            "SpearmanrResult(correlation=0.634583877990814, pvalue=0.0)\n",
            "Epoch 57/60\n",
            "200/200 [==============================] - 28s 140ms/step - loss: 0.3306 - val_loss: 0.4374\n",
            "SpearmanrResult(correlation=0.6277370340575363, pvalue=0.0)\n",
            "Epoch 58/60\n",
            "200/200 [==============================] - 28s 139ms/step - loss: 0.3314 - val_loss: 0.4530\n",
            "SpearmanrResult(correlation=0.627750671016841, pvalue=0.0)\n",
            "Epoch 59/60\n",
            "200/200 [==============================] - 28s 141ms/step - loss: 0.3228 - val_loss: 0.4643\n",
            "SpearmanrResult(correlation=0.6197709055710714, pvalue=0.0)\n",
            "Epoch 60/60\n",
            "200/200 [==============================] - 27s 136ms/step - loss: 0.3262 - val_loss: 0.4585\n",
            "SpearmanrResult(correlation=0.6339969605843355, pvalue=0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0zY2wgjqywY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e469e420-4978-4691-814a-3a76ea3ab888"
      },
      "cell_type": "code",
      "source": [
        "#compute auROC and auPRC\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from avutils import util\n",
        "import numpy as np\n",
        "\n",
        "test_set_generator = pyfasta_data_loader.SingleStreamSeqOnly(\n",
        "     batch_size=None,\n",
        "     bed_source=\"test_positives_asinh_spp.gz\",\n",
        "     fasta_data_source=\"hg19.genome.fa\",\n",
        "     rc_augment=True,\n",
        "     num_to_load_for_eval=None, #not used\n",
        "     labels_dtype=\"float\",\n",
        "     wrap_in_keys=[\"sequence\", \"output\"],\n",
        "     randomize_after_pass=False).get_generator(loop_infinitely=False)\n",
        "\n",
        "positives_asinh_spp_preds = []\n",
        "true_test_labels = []\n",
        "\n",
        "X_batch = []\n",
        "for (X_ex, Y_ex, coor, fastastr) in test_set_generator:\n",
        "  true_test_labels.append(Y_ex)\n",
        "  if (len(true_test_labels)%1000 == 0):\n",
        "    print(\"Done\",len(true_test_labels))\n",
        "  X_batch.append(X_ex)\n",
        "  if (len(X_batch)==200):\n",
        "    X_batch = {\"sequence\": np.array(X_batch)}\n",
        "    positives_asinh_spp_preds.extend(positives_asinh_spp_model.predict(X_batch))\n",
        "    X_batch = []\n",
        "#leftover examples at the end that didn't fit in a batch\n",
        "if (len(X_batch) > 0):\n",
        "  X_batch = {\"sequence\": np.array(X_batch)}\n",
        "  positives_asinh_spp_preds.extend(positives_asinh_spp_model.predict(X_batch))\n",
        "\n",
        "positives_asinh_spp_preds = np.array(positives_asinh_spp_preds)\n",
        "true_test_labels = np.array(true_test_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading bed file test_positives_asinh_spp.gz into memory\n",
            "Finished reading bed file into memory; got 3466rows\n",
            "Done 1000\n",
            "Done 2000\n",
            "Done 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: generator 'get_pyfasta_generator' raised StopIteration\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "h01VlA6M76cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "682c3aef-0db5-4445-b822-b4fc4f498fee"
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "print(spearmanr(a=true_test_labels[:,0],\n",
        "                b=positives_asinh_spp_preds[:,0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpearmanrResult(correlation=0.6333029683424352, pvalue=0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4dm-61P77mM7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}